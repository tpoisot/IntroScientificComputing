<!doctype html><html><head><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.1/css/all.css integrity=sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP crossorigin=anonymous><link rel=stylesheet href=//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.css><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.js></script><link rel=stylesheet href=/css/normalize.css><link rel=stylesheet href=https://sciencecomputing.io/configuration.3f624b39616217ce894cfbc7c681faa5c3287234e401161b5d830595888d8a0f.css><link rel=stylesheet href=https://sciencecomputing.io/index.b5d7af4aaf50f86cc4e1232bfbc8cd98a64f22aec70358c9b0ff54cc009efc49.css><meta charset=utf-8><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js integrity=sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false}]});});</script><title>Naive Bayes classifier · Scientific computing</title></head><body><nav id=main class=sidebar><img src=/logo.png alt style=position:absolute;left:50%;width:56px;margin-left:-28px;margin-top:5px><div class=header><h1><a href=/>Scientific computing</a></h1><h2>(for the rest of us)</h2></div><ul class=nav><li class=lessons><i class="fas fa-fw fa-book-open"></i><a class=sidebar-nav-item href=/lessons/>Lessons</a></li><li class=primers><i class="fas fa-fw fa-bolt"></i><a class=sidebar-nav-item href=/primers/>Primers</a></li><li class=capstones><i class="fas fa-fw fa-star"></i><a class=sidebar-nav-item href=/capstones/>Capstones</a></li><li class=ml><i class="fas fa-fw fa-brain"></i><a class=sidebar-nav-item href=/machinelearning/>AI/ML</a></li></ul></nav><section id=content><section class=introduction><p>In this capstone, we will implement a Naive Bayes classifier, to make
predictions about the cultivar to which wheat seeds belong. This is a
classification task, meaning that we want to get an answer that looks like
<code>class A</code> or <code>class B</code>, as opposed to a regression problem in which we would
like to get an answer like <code>length = 0.8cm</code>. At its core, Naive Bayes
classification assumes that the probability of belonging to a class based on a
single measurement is equal to the probability that this measurement&rsquo;s values originate
from the distribution of known values for the class. For example, we know that the
road runner (<em>Tastyus supersonicus</em>) runs at a speed of 25.8 ± 1.2 mph, whereas
the coyote (<em>Eatius birdius</em>) runs at a speed of 21.5 ± 1.8 mph. If we measure
an unidentified animal going 24.6 mph in a area where coyotes are three quarters
of the total population, we can use NBC to get the probability that it is a
coyote or a roadrunner:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=k>using</span> <span class=n>Distributions</span>

<span class=n>speed_coyote</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>21.5</span><span class=p>,</span> <span class=mf>1.8</span><span class=p>)</span>
<span class=n>speed_roadrunner</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>25.8</span><span class=p>,</span> <span class=mf>1.2</span><span class=p>)</span>
<span class=n>measured_speed</span> <span class=o>=</span> <span class=mf>24.6</span>

<span class=n>P_roadrunner</span> <span class=o>=</span> <span class=mf>0.25</span> <span class=o>*</span> <span class=n>pdf</span><span class=p>(</span><span class=n>speed_roadrunner</span><span class=p>,</span> <span class=n>measured_speed</span><span class=p>)</span>
<span class=n>P_coyote</span> <span class=o>=</span> <span class=mf>0.75</span> <span class=o>*</span> <span class=n>pdf</span><span class=p>(</span><span class=n>speed_coyote</span><span class=p>,</span> <span class=n>measured_speed</span><span class=p>)</span>

<span class=p>[</span><span class=o>:</span><span class=n>roadrunner</span><span class=p>,</span> <span class=o>:</span><span class=n>coyote</span><span class=p>][</span><span class=n>argmax</span><span class=p>([</span><span class=n>P_roadrunner</span><span class=p>,</span> <span class=n>P_coyote</span><span class=p>])]</span>
</code></pre></div><pre><code>:roadrunner
</code></pre><p>The <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Wikipedia page</a> has a very clear introduction to the theory, and we
will adhere to its notation here. In short, given an array $\mathbf{x}$ of
observations of multiple features, the probability ($p(C_k | x)$) that this
sample belongs to the class $C_k$ is <em>proportional</em> to $p(C_k) \prod p(x_i |
C_k)$. We can multiply these probabilities together because Naive Bayes
classification makes the assumption of independance between features. This is
precisely why we say that this classifier is &lsquo;naive&rsquo;. Further, we can get the
class to which the sample should be assigned using $\hat y = \text{argmax}_{k
\in K} p(C_k) \prod p(x_i | C_k)$ - the class that is the most probable is taken
as the guess.</p><p>Calculating $p(x_i | C_k)$ assumes that we know the distribution of the values
of the <em>i</em>-th variable in class $C_k$, which is not necessarily true. In this
example we will assume that all we know for sure is the average and standard
deviation, meaning that the distribution we will use should be a normal
distribution, which has the maximal entropy given the information available.</p><p>We will re-use the dataset from the <a href=https://sciencecomputing.io/machinelearning/neural_network_flux/>neural network with <em>Flux</em></a>
capstone, and so we will skip over the details of how these data can be
downloaded:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=k>import</span> <span class=n>CSV</span>
<span class=k>using</span> <span class=n>DataFrames</span>

<span class=k>function</span> <span class=n>get_dataset</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>filename</span><span class=p>)</span>
	<span class=k>if</span> <span class=o>!</span><span class=n>isfile</span><span class=p>(</span><span class=n>filename</span><span class=p>)</span>
		<span class=n>download</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>filename</span><span class=p>)</span>
	<span class=k>end</span>
	<span class=k>return</span> <span class=n>dropmissing</span><span class=p>(</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>CSV</span><span class=o>.</span><span class=n>File</span><span class=p>(</span><span class=n>filename</span><span class=p>;</span> <span class=n>header</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>delim</span><span class=o>=</span><span class=sc>&#39;\t&#39;</span><span class=p>)))</span>
<span class=k>end</span>
<span class=kd>const</span> <span class=n>seed_url</span> <span class=o>=</span> <span class=s>&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt&#34;</span>
<span class=n>seeds</span> <span class=o>=</span> <span class=n>get_dataset</span><span class=p>(</span><span class=n>seed_url</span><span class=p>,</span> <span class=s>&#34;seeds.txt&#34;</span><span class=p>);</span>
</code></pre></div><p>This dataset comprises a bunch of measures taken on wheat kernels.
The kernels belong to three different cultivars of wheat.
As always, we will rename the columns to make it easier to reference them:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>rename!</span><span class=p>(</span><span class=n>seeds</span><span class=p>,</span>
  <span class=p>[</span><span class=o>:</span><span class=n>Column1</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>area</span><span class=p>,</span> <span class=o>:</span><span class=n>Column2</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>perimeter</span><span class=p>,</span>
  <span class=o>:</span><span class=n>Column3</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>compactness</span><span class=p>,</span> <span class=o>:</span><span class=n>Column4</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>kernel_length</span><span class=p>,</span>
  <span class=o>:</span><span class=n>Column5</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>kernel_width</span><span class=p>,</span> <span class=o>:</span><span class=n>Column6</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>asymmetry</span><span class=p>,</span>
  <span class=o>:</span><span class=n>Column7</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>kernel_groove</span><span class=p>,</span> <span class=o>:</span><span class=n>Column8</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>cultivar</span><span class=p>]</span>
  <span class=p>)</span>

<span class=k>using</span> <span class=n>Random</span>
<span class=n>Random</span><span class=o>.</span><span class=n>seed!</span><span class=p>(</span><span class=mi>42</span><span class=p>);</span>
<span class=n>seeds</span> <span class=o>=</span> <span class=n>seeds</span><span class=p>[</span><span class=n>shuffle</span><span class=p>(</span><span class=mi>1</span><span class=o>:</span><span class=k>end</span><span class=p>),</span> <span class=o>:</span><span class=p>]</span>
<span class=n>first</span><span class=p>(</span><span class=n>seeds</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>3×8 DataFrame
 Row │ area     perimeter  compactness  kernel_length  kernel_width  asymme
try ⋯
     │ Float64  Float64    Float64      Float64        Float64       Float6
4   ⋯
─────┼─────────────────────────────────────────────────────────────────────
─────
   1 │   17.32      15.91       0.8599          6.064         3.403      3.
824 ⋯
   2 │   17.08      15.38       0.9079          5.832         3.683      2.
956
   3 │   12.79      13.53       0.8786          5.224         3.054      5.
483
                                                               2 columns om
itted
</code></pre><p>To see how well our technique performs, we will split the dataset in two,
keeping 20 samples for the validation, and the rest to get information on the
distributions:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>seeds_dist</span> <span class=o>=</span> <span class=n>seeds</span><span class=p>[</span><span class=mi>1</span><span class=o>:</span><span class=p>(</span><span class=k>end</span><span class=o>-</span><span class=mi>20</span><span class=p>),</span><span class=o>:</span><span class=p>];</span>
<span class=n>leftovers</span> <span class=o>=</span> <span class=n>seeds</span><span class=p>[(</span><span class=k>end</span><span class=o>-</span><span class=mi>19</span><span class=p>)</span><span class=o>:</span><span class=k>end</span><span class=p>,</span><span class=o>:</span><span class=p>];</span>
</code></pre></div><p>As it is, our first task is to summarize the data in <code>seeds_dist</code> in a table
containing, for every measure and every cultivar, the average and standard
deviation. The next steps will involve a fair amount of manipulations using the
<code>DataFrames</code> package, and it is a good idea to look at the documentation for the
various functions. Note that using a dataframe here is absolutely not required,
but in the process of shaping our data, we will use the most common
transformations.</p><p>Because we have <em>a lot</em> of variables, it is likely easier to reshape
the data to a long format:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>long_seeds</span> <span class=o>=</span> <span class=n>stack</span><span class=p>(</span><span class=n>seeds_dist</span><span class=p>,</span> <span class=n>Not</span><span class=p>(</span><span class=o>:</span><span class=n>cultivar</span><span class=p>))</span>
<span class=n>first</span><span class=p>(</span><span class=n>long_seeds</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>3×3 DataFrame
 Row │ cultivar  variable  value
     │ Float64   String    Float64
─────┼─────────────────────────────
   1 │      2.0  area        17.32
   2 │      1.0  area        17.08
   3 │      3.0  area        12.79
</code></pre><p>We can now calculate the mean and standard deviation for all combinations of
variables and cultivars:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=k>using</span> <span class=n>Statistics</span>
<span class=n>distribution_data</span> <span class=o>=</span> <span class=n>by</span><span class=p>(</span><span class=n>long_seeds</span><span class=p>,</span> <span class=n>Not</span><span class=p>(</span><span class=o>:</span><span class=n>value</span><span class=p>),</span> <span class=n>μ</span> <span class=o>=</span> <span class=o>:</span><span class=n>value</span> <span class=o>=&gt;</span> <span class=n>mean</span><span class=p>,</span> <span class=n>σ</span> <span class=o>=</span> <span class=o>:</span><span class=n>value</span> <span class=o>=&gt;</span> <span class=n>std</span><span class=p>)</span>
<span class=n>first</span><span class=p>(</span><span class=n>distribution_data</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>Error: ArgumentError: by function was removed from DataFrames.jl. Use the `
combine(groupby(...), ...)` or `combine(f, groupby(...))` instead.
</code></pre><p>This is the sort of table one can find in a publication - in fact, the greatness
of Naive Bayes classification is that it can work even if you only have access
to the moments of the distribution, and not to the raw data themselves!</p><p>We now need to create statistical distributions from the information in this
table - in <em>Julia</em>, a dataframe can store just about any type of information,
so it is perfectly fine to store the object representing the normal distribution
for each variable and cultivar in a new column.</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>distribution_data</span><span class=o>.</span><span class=n>distribution</span> <span class=o>=</span> <span class=p>[</span><span class=n>Normal</span><span class=p>(</span><span class=n>row</span><span class=o>.</span><span class=n>μ</span><span class=p>,</span> <span class=n>row</span><span class=o>.</span><span class=n>σ</span><span class=p>)</span> <span class=k>for</span> <span class=n>row</span> <span class=kp>in</span> <span class=n>eachrow</span><span class=p>(</span><span class=n>distribution_data</span><span class=p>)];</span>
</code></pre></div><pre><code>Error: UndefVarError: distribution_data not defined
</code></pre><p>Of course, this format is not ideal for what we&rsquo;re about to do (measure the
probability that a sample belongs to a class), so we will unstack it by cultivar
(this drops the <code>μ</code> and <code>σ</code> columns, but they are not needed anymore):</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>distributions</span> <span class=o>=</span> <span class=n>unstack</span><span class=p>(</span><span class=n>distribution_data</span><span class=p>,</span> <span class=o>:</span><span class=n>cultivar</span><span class=p>,</span> <span class=o>:</span><span class=n>variable</span><span class=p>,</span> <span class=o>:</span><span class=n>distribution</span><span class=p>)</span>
<span class=n>select</span><span class=p>(</span><span class=n>distributions</span><span class=p>,</span> <span class=p>[</span><span class=o>:</span><span class=n>cultivar</span><span class=p>,</span> <span class=o>:</span><span class=n>area</span><span class=p>,</span> <span class=o>:</span><span class=n>compactness</span><span class=p>])</span>
</code></pre></div><pre><code>Error: UndefVarError: distribution_data not defined
</code></pre><p>And with this information, we are ready to make a prediction. We will keep the
cultivar information from the <code>leftovers</code> dataset, and then remove it to
simulate what would happen if we had, for example, spilled all of our wheat
kernels and had to put them in the correct kernel bags.</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>true_cultivar</span> <span class=o>=</span> <span class=n>leftovers</span><span class=o>.</span><span class=n>cultivar</span>
<span class=n>select!</span><span class=p>(</span><span class=n>leftovers</span><span class=p>,</span> <span class=n>Not</span><span class=p>(</span><span class=o>:</span><span class=n>cultivar</span><span class=p>))</span>
<span class=n>first</span><span class=p>(</span><span class=n>leftovers</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>3×7 DataFrame
 Row │ area     perimeter  compactness  kernel_length  kernel_width  asymme
try ⋯
     │ Float64  Float64    Float64      Float64        Float64       Float6
4   ⋯
─────┼─────────────────────────────────────────────────────────────────────
─────
   1 │   13.2       13.66       0.8883          5.236         3.232      8.
315 ⋯
   2 │   15.03      14.77       0.8658          5.702         3.212      1.
933
   3 │   15.78      14.91       0.8923          5.674         3.434      5.
593
                                                                1 column om
itted
</code></pre><p>Let&rsquo;s take the first row. We want to get the probability of every observation being
taken from the distribution of this variable for every cultivar. The probability
that this sample comes from cultivar 2 knowing its area is:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>pdf</span><span class=p>(</span><span class=n>distributions</span><span class=o>.</span><span class=n>area</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>leftovers</span><span class=o>.</span><span class=n>area</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</code></pre></div><pre><code>Error: UndefVarError: distributions not defined
</code></pre><p>To do this for all samples, we will assign a unique <code>id</code> to our observations for
the test, and then stack both dataframes by variable;</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>leftovers</span><span class=o>.</span><span class=n>id</span> <span class=o>=</span> <span class=mi>1</span><span class=o>:</span><span class=n>size</span><span class=p>(</span><span class=n>leftovers</span><span class=p>,</span><span class=mi>1</span><span class=p>)</span>
<span class=n>s_leftovers</span> <span class=o>=</span> <span class=n>stack</span><span class=p>(</span><span class=n>leftovers</span><span class=p>,</span> <span class=n>Not</span><span class=p>(</span><span class=o>:</span><span class=n>id</span><span class=p>))</span>
<span class=n>s_distributions</span> <span class=o>=</span> <span class=n>stack</span><span class=p>(</span><span class=n>distributions</span><span class=p>,</span> <span class=n>Not</span><span class=p>(</span><span class=o>:</span><span class=n>cultivar</span><span class=p>))</span>
<span class=n>rename!</span><span class=p>(</span><span class=n>s_distributions</span><span class=p>,</span> <span class=o>:</span><span class=n>value</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>distribution</span><span class=p>)</span>
</code></pre></div><pre><code>Error: UndefVarError: distributions not defined
</code></pre><p>These dataframes have one identical column (<code>:variable</code>), which we can use to
<code>join</code> them:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>s_merged</span> <span class=o>=</span> <span class=n>join</span><span class=p>(</span><span class=n>s_leftovers</span><span class=p>,</span> <span class=n>s_distributions</span><span class=p>,</span> <span class=n>on</span><span class=o>=:</span><span class=n>variable</span><span class=p>)</span>
<span class=n>first</span><span class=p>(</span><span class=n>s_merged</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>Error: UndefVarError: s_distributions not defined
</code></pre><p>Finally, because we consider all variables independently, we can now get the
probability of every measurement coming from its appropriate distribution:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>s_proba</span> <span class=o>=</span> <span class=n>by</span><span class=p>(</span><span class=n>s_merged</span><span class=p>,</span> <span class=p>[</span><span class=o>:</span><span class=n>cultivar</span><span class=p>,</span> <span class=o>:</span><span class=n>id</span><span class=p>,</span> <span class=o>:</span><span class=n>variable</span><span class=p>],</span>
	<span class=p>[</span><span class=o>:</span><span class=n>value</span><span class=p>,</span> <span class=o>:</span><span class=n>distribution</span><span class=p>]</span> <span class=o>=&gt;</span>  <span class=n>x</span> <span class=o>-&gt;</span> <span class=p>(</span><span class=n>p</span> <span class=o>=</span> <span class=n>pdf</span><span class=o>.</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>distribution</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>value</span><span class=p>))</span>
	<span class=p>)</span>
<span class=n>rename!</span><span class=p>(</span><span class=n>s_proba</span><span class=p>,</span> <span class=o>:</span><span class=n>x1</span> <span class=o>=&gt;</span> <span class=o>:</span><span class=n>probability</span><span class=p>)</span>
<span class=n>first</span><span class=p>(</span><span class=n>s_proba</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div><pre><code>Error: UndefVarError: s_merged not defined
</code></pre><p>The very last step is to bring everything back together, and to multiply the
probabilities for every variable <em>within</em> every cultivar together, and get the
argmax (most probable class):</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>s_p</span> <span class=o>=</span> <span class=n>by</span><span class=p>(</span><span class=n>s_proba</span><span class=p>,</span> <span class=p>[</span><span class=o>:</span><span class=n>cultivar</span><span class=p>,</span> <span class=o>:</span><span class=n>id</span><span class=p>],</span> <span class=n>p</span> <span class=o>=</span> <span class=o>:</span><span class=n>probability</span> <span class=o>=&gt;</span> <span class=n>prod</span><span class=p>)</span>
<span class=n>s_argmax</span> <span class=o>=</span> <span class=n>by</span><span class=p>(</span><span class=n>s_p</span><span class=p>,</span> <span class=o>:</span><span class=n>id</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=o>:</span><span class=n>p</span> <span class=o>=&gt;</span> <span class=n>argmax</span><span class=p>)</span>
</code></pre></div><pre><code>Error: UndefVarError: s_proba not defined
</code></pre><p>This leaves us with a column containing the class we guessed, and we can compare
it to the actual class to get the accuracy:</p><div class=highlight><pre class=chroma><code class=language-julia data-lang=julia><span class=n>accuracy</span> <span class=o>=</span> <span class=n>mean</span><span class=p>(</span><span class=n>s_argmax</span><span class=o>.</span><span class=n>y</span> <span class=o>.==</span> <span class=n>true_cultivar</span><span class=p>)</span>
</code></pre></div><pre><code>Error: UndefVarError: s_argmax not defined
</code></pre><p>Naive Bayes classifiers have uncanny accuracy on a lot of problems! This example
relies on dataframes to reach the answer, but of course it is a valuable
programming exercise to do it starting from a matrix of values.</p></section></section><script>mermaid.initialize({startOnLoad:true,theme:"neutral",curve:"basis"});</script></body></html>