<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistics on</title><link>http://sciencecomputing.io/packages/statistics/</link><description>Recent content in Statistics on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://sciencecomputing.io/packages/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Approximate Bayesian Computation</title><link>http://sciencecomputing.io/capstones/abc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/capstones/abc/</guid><description>Approximate Bayesian computation, or ABC for short, is a very useful heuristic to estimate the posterior distribution of model parameters, specifically when the analytical expression of the likelihood function is unavailable (or when we can't be bothered to figure it out). The theory on how ABC works will not be covered here in detail, so reading the previous article is highly recommended.
We will rely on a few packages for this example:</description></item><item><title>Genetic algorithm</title><link>http://sciencecomputing.io/capstones/genetic_algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/capstones/genetic_algorithm/</guid><description>Genetic algorithm is a heuristic that takes heavy inspiration from evolutionary biology, to explore a space of parameters rapidly and converge to an optimum. Every solution is a &amp;ldquo;genome&amp;rdquo;, and the combinations can undergo mutation and recombination. By simulating a process of reproduction, over sufficiently many generation, this heuristic usually gives very good results. It is also simple to implement, and this is what we will do!
A genetic algorithm works by measuring the fitness of a solution (i.</description></item><item><title>Neural network with Flux</title><link>http://sciencecomputing.io/machinelearning/neuralnetwork/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/machinelearning/neuralnetwork/</guid><description>short intro text goes here
using Flux ERROR: On worker 2: ArgumentError: Package Flux &amp;#91;587475ba-b771-5e3f-ad9e-33799f191a9c&amp;#93; is required but does not seem to be installed: - Run &amp;#96;Pkg.instantiate&amp;#40;&amp;#41;&amp;#96; to install all recorded dependencies. _require at ./loading.jl:993 require at ./loading.jl:922 #1 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.3/Distributed/src/Distributed.jl:78 #105 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.3/Distributed/src/process_messages.jl:290 run_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.3/Distributed/src/process_messages.jl:79 run_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.3/Distributed/src/process_messages.jl:88 #98 at ./task.jl:333 ...and 1 more exception&amp;#40;s&amp;#41;. import CSV using DataFrames using Random using Statistics The first thing we need to do is download our data, which we will store in our local folder.</description></item><item><title>Neural Bayes classifier</title><link>http://sciencecomputing.io/machinelearning/naivebayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/machinelearning/naivebayes/</guid><description>In this capstone, we will implement a Naive Bayes classifier, to make predictions about the cultivar to which wheat seeds belong. This is a classification task, meaning that we want to get an answer that looks like class A or class B, as opposed to a regression problem in which we would like to get an answer like length = 0.8cm. At its core, Naive Bayes classification assumes that the probability of belonging to a class based on a single measurement is equal to the probability that this measurement's values originate from the distribution of known values for the class.</description></item><item><title>The flow of execution</title><link>http://sciencecomputing.io/lessons/control_flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/lessons/control_flow/</guid><description>Programming really is a language But if you understand three words, you will be able to hold a good conversation with your computer! These three words are if, for, and while. If you have some previous experience with writing code, you can skim through this lesson.
One great way to make your code robust is to keep it very simple, and one great way to keep your code very simple is to recognize that often, we want to do one of three things: do one thing if something happens (if), do one thing to a series of things (for), or do one thing until something happens (while).</description></item><item><title>Avoiding mistakes</title><link>http://sciencecomputing.io/lessons/avoiding_mistakes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/lessons/avoiding_mistakes/</guid><description>We can't avoid mistakes But we can work as cautiously as possible, to make sure we catch them in time. It is always better to try and fail to run something, than to have the operation keep going and accumulating mistakes.
There are four types of mistakes to look out for: mistakes in the code, confusing interface, issues with arguments, and lack of integration. Some are caused by the programmer, and some are caused by the user.</description></item></channel></rss>