<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributions on</title><link>http://sciencecomputing.io/packages/distributions/</link><description>Recent content in Distributions on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://sciencecomputing.io/packages/distributions/index.xml" rel="self" type="application/rss+xml"/><item><title>Approximate Bayesian Computation</title><link>http://sciencecomputing.io/capstones/abc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/capstones/abc/</guid><description>Approximate Bayesian computation, or ABC for short, is a very useful heuristic to estimate the posterior distribution of model parameters, specifically when the analytical expression of the likelihood function is unavailable (or when we can't be bothered to figure it out). The theory on how ABC works will not be covered here in detail, so reading the previous article (and the references it links to) is highly recommended.
We will rely on a few packages for this example:</description></item><item><title>Naive Bayes classifier</title><link>http://sciencecomputing.io/machinelearning/naivebayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/machinelearning/naivebayes/</guid><description>In this capstone, we will implement a Naive Bayes classifier, to make predictions about the cultivar to which wheat seeds belong. This is a classification task, meaning that we want to get an answer that looks like class A or class B, as opposed to a regression problem in which we would like to get an answer like length = 0.8cm. At its core, Naive Bayes classification assumes that the probability of belonging to a class based on a single measurement is equal to the probability that this measurement's values originate from the distribution of known values for the class.</description></item></channel></rss>