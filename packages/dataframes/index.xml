<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DataFrames on</title><link>http://sciencecomputing.io/packages/dataframes/</link><description>Recent content in DataFrames on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://sciencecomputing.io/packages/dataframes/index.xml" rel="self" type="application/rss+xml"/><item><title>Genetic algorithm</title><link>http://sciencecomputing.io/capstones/genetic_algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/capstones/genetic_algorithm/</guid><description>Genetic algorithm is a heuristic that takes heavy inspiration from evolutionary biology, to explore a space of parameters rapidly and converge to an optimum. Every solution is a &amp;ldquo;genome&amp;rdquo;, and the combinations can undergo mutation and recombination. By simulating a process of reproduction, over sufficiently many generation, this heuristic usually gives very good results. It is also simple to implement, and this is what we will do!
A genetic algorithm works by measuring the fitness of a solution (i.</description></item><item><title>Neural network with Flux</title><link>http://sciencecomputing.io/machinelearning/neuralnetwork/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/machinelearning/neuralnetwork/</guid><description>short intro text goes here
using Flux import CSV using DataFrames using Random using Statistics The first thing we need to do is download our data, which we will store in our local folder. The data are available from the UCI Machine Learning repository. To make sure that we do not download the dataset more often than necessary, we will write a short function to download the dataset if it doesn't exist, and if it exists, to return it as a data frame:</description></item><item><title>Neural Bayes classifier</title><link>http://sciencecomputing.io/machinelearning/naivebayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://sciencecomputing.io/machinelearning/naivebayes/</guid><description>In this capstone, we will implement a Naive Bayes classifier, to make predictions about the cultivar to which wheat seeds belong. This is a classification task, meaning that we want to get an answer that looks like class A or class B, as opposed to a regression problem in which we would like to get an answer like length = 0.8cm. At its core, Naive Bayes classification assumes that the probability of belonging to a class based on a single measurement is equal to the probability that this measurement's values originate from the distribution of known values for the class.</description></item></channel></rss>